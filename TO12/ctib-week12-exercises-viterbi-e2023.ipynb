{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # Just ignore this :-)\n",
    "\n",
    "def log(x):\n",
    "    if x == 0:\n",
    "        return float('-inf')\n",
    "    return math.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTiB E2023 - Week 11 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical exercises\n",
    "\n",
    "***Exercise 1***: How many terms are there in the sum on slide 37 from the lecture on Nov 13 for computing $P({\\bf X}|\\Theta)$? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:magenta\">Når man skal finde summen af sandsynligheder for en bestemt HMM, starter man først i Z1. Det tager 1 at finde den værdi (fra inital prob), for at finde x1, skal man bruge z1 og hente værdien af emission prob, det tager altså også 1.\n",
    "Når man skal finde alle efterfølgende z afhænger de at z før - tager altså også 1. Det samme gælde efterfølgende x, hvor det afhænger af den tilsvarende z.\n",
    "term = hver gang man skal hente en sandsynlighed. \n",
    "Altså er summen af terms på slide 37 N*2 (N = længden af den observeret sekvens og x2 fordi der både skal bruges transition og emission)\n",
    "Fra gennemgang: K^N. \n",
    "</span>\n",
    "\n",
    "![[joint prob.png]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 2***: How many terms are there in the maximization on slide 4 from the lecure on Nov 20 for computing the Viterbi decoding ${\\bf Z}^*$? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:magenta\">\n",
    "Viterbi decoding: Man prøver at finde Z imellem alle Z (Z=K^N) der maximerer joint probability. \n",
    "\n",
    "Vi har N*K matrix, hvor N er længden af de observeret states og K er antallet af hidden state, betydende at k er antallet af rækker.\n",
    "Når vi skal udfylde den første kolonne i vores matrix, skal vi gange initial prob og emission prob sammen - det tager altså 2 terms at udfylde et felt i vores matrix => det tager k*2 terms at udfylde den første kolonne.\n",
    "De efterfølgende kolonner: Så for at udfylde et felt skal man gange sandsynligheden for at være i det state efter det forrig state, det udregnes ved at gange emission og transition prob. Man får da k*2. For hver pil tilføjes k*2 terms og max tager 1 term, vi får da k*2+1*k. \n",
    "\n",
    "Det vil altså tage $2\\cdot k+(N-1)\\cdot(k\\cdot(k\\cdot 2+1))$\n",
    "\n",
    "(N-1 fordi den første kolonne udregnes anderledes end resten)\n",
    "</span>\n",
    "\n",
    "Fra gennemgang: K^N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3***: Where in the derivation of $\\omega({\\bf z}_n$) on slide 8 do we use that the fact that we are working with hidden Markov models? And how do we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fra step 1 til step 2, hvor man begynder at dele udtrykket op, så man kan skrive hvordan ting afhænger af hinanden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4:\n",
    "Fra gennemgang: vi skal gå igennem hver værdi af x i x1-xN, så ?\n",
    "\n",
    "Exercise 5:\n",
    "\n",
    "Exercise 6:\n",
    "K*2 fordi man kun skal gemme to kolonner ad gangen for at udfylde tabellen da n kun afhænger af den forrige kolonne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical exercises\n",
    "\n",
    "You are given the same 7-state HMM and helper functions that you used last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hmm:\n",
    "    def __init__(self, init_probs, trans_probs, emission_probs):\n",
    "        self.init_probs = init_probs\n",
    "        self.trans_probs = trans_probs\n",
    "        self.emission_probs = emission_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_probs_7_state = [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00]\n",
    "\n",
    "trans_probs_7_state = [\n",
    "    [0.00, 0.00, 0.90, 0.10, 0.00, 0.00, 0.00],\n",
    "    [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.05, 0.90, 0.05, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
    "    [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
    "    [0.00, 0.00, 0.00, 0.10, 0.90, 0.00, 0.00],\n",
    "]\n",
    "\n",
    "emission_probs_7_state = [\n",
    "    #   A     C     G     T\n",
    "    [0.30, 0.25, 0.25, 0.20],\n",
    "    [0.20, 0.35, 0.15, 0.30],\n",
    "    [0.40, 0.15, 0.20, 0.25],\n",
    "    [0.25, 0.25, 0.25, 0.25],\n",
    "    [0.20, 0.40, 0.30, 0.10],\n",
    "    [0.30, 0.20, 0.30, 0.20],\n",
    "    [0.15, 0.30, 0.20, 0.35],\n",
    "]\n",
    "\n",
    "hmm_7_state = hmm(init_probs_7_state, trans_probs_7_state, emission_probs_7_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_observations_to_indices(obs):\n",
    "    mapping = {'a': 0, 'c': 1, 'g': 2, 't': 3}\n",
    "    return [mapping[symbol.lower()] for symbol in obs]\n",
    "\n",
    "def translate_indices_to_observations(indices):\n",
    "    mapping = ['a', 'c', 'g', 't']\n",
    "    return ''.join(mapping[idx] for idx in indices)\n",
    "\n",
    "def translate_path_to_indices(path):\n",
    "    return list(map(lambda x: int(x), path))\n",
    "\n",
    "def translate_indices_to_path(indices):\n",
    "    return ''.join([str(i) for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1 - Viterbi Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will implement and experiment with the Viterbi algorithm. The implementation has been split into three parts:\n",
    "\n",
    "1. Fill out the $\\omega$ table using the recursion presented at the lecture.\n",
    "2. Find the state with the highest probability after observing the entire sequence of observations.\n",
    "3. Backtrack from the state found in the previous step to obtain the optimal path.\n",
    "\n",
    "We'll be working with the 7-state model (`hmm_7_state`) and the helper function for translating between observations, hidden states, and indicies, as introduced above (and also used last week)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you're given the function below that constructs a table of a specific size filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_table(m, n):\n",
    "    \"\"\"Make a table with `m` rows and `n` columns filled with zeros.\"\"\"\n",
    "    return [[0] * n for _ in range(m)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be testing your code with the same two sequences as last week, i.e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_short = 'GTTTCCCAGTGTATATCGAGGGATACTACGTGCATAGTAACATCGGCCAA'\n",
    "z_short = '33333333333321021021021021021021021021021021021021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_long = 'TGAGTATCACTTAGGTCTATGTCTAGTCGTCTTTCGTAATGTTTGGTCTTGTCACCAGTTATCCTATGGCGCTCCGAGTCTGGTTCTCGAAATAAGCATCCCCGCCCAAGTCATGCACCCGTTTGTGTTCTTCGCCGACTTGAGCGACTTAATGAGGATGCCACTCGTCACCATCTTGAACATGCCACCAACGAGGTTGCCGCCGTCCATTATAACTACAACCTAGACAATTTTCGCTTTAGGTCCATTCACTAGGCCGAAATCCGCTGGAGTAAGCACAAAGCTCGTATAGGCAAAACCGACTCCATGAGTCTGCCTCCCGACCATTCCCATCAAAATACGCTATCAATACTAAAAAAATGACGGTTCAGCCTCACCCGGATGCTCGAGACAGCACACGGACATGATAGCGAACGTGACCAGTGTAGTGGCCCAGGGGAACCGCCGCGCCATTTTGTTCATGGCCCCGCTGCCGAATATTTCGATCCCAGCTAGAGTAATGACCTGTAGCTTAAACCCACTTTTGGCCCAAACTAGAGCAACAATCGGAATGGCTGAAGTGAATGCCGGCATGCCCTCAGCTCTAAGCGCCTCGATCGCAGTAATGACCGTCTTAACATTAGCTCTCAACGCTATGCAGTGGCTTTGGTGTCGCTTACTACCAGTTCCGAACGTCTCGGGGGTCTTGATGCAGCGCACCACGATGCCAAGCCACGCTGAATCGGGCAGCCAGCAGGATCGTTACAGTCGAGCCCACGGCAATGCGAGCCGTCACGTTGCCGAATATGCACTGCGGGACTACGGACGCAGGGCCGCCAACCATCTGGTTGACGATAGCCAAACACGGTCCAGAGGTGCCCCATCTCGGTTATTTGGATCGTAATTTTTGTGAAGAACACTGCAAACGCAAGTGGCTTTCCAGACTTTACGACTATGTGCCATCATTTAAGGCTACGACCCGGCTTTTAAGACCCCCACCACTAAATAGAGGTACATCTGA'\n",
    "z_long = '3333321021021021021021021021021021021021021021021021021021021021021021033333333334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210210321021021021021021021021033334564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564564563333333456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456456332102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102102103210210210210210210210210210210210210210210210210210210210210210'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to translate these sequences to indices before using them with your algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing without log-transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will implement the algorithm without log-transformation. This will cause issues with numerical stability (like above when computing the joint probability), so we will use the log-transformation trick to fix this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the $\\omega$ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0, 0, 0.0001875, 6.328125000000001e-05, 7.910156250000001e-06, 2.53125e-06], [0.0, 0, 0.0009375, 0.00021093750000000002, 3.1640625000000005e-05, 1.0125e-05, 3.986718750000001e-06], [0.0, 0.003125, 0.0007031250000000001, 0.00015820312500000003, 6.75e-05, 1.1390625000000002e-05, 1.0812194824218752e-06], [0.25, 0.05625, 0.01265625, 0.0028476562500000004, 0.0006407226562500001, 0.00014416259765625004, 3.243658447265626e-05], [0.0, 0.0012500000000000002, 0.00028125000000000003, 6.328125000000001e-05, 2.8476562500000005e-05, 9.610839843750001e-06, 2.883251953125001e-06], [0.0, 0, 0.00025000000000000006, 5.625000000000001e-05, 1.8984375000000002e-05, 8.54296875e-06, 1.9221679687500005e-06], [0.0, 0, 0, 8.750000000000001e-05, 8.437500000000002e-06, 3.796875000000001e-06, 2.5628906250000003e-06]]\n"
     ]
    }
   ],
   "source": [
    "def compute_w(model, x):\n",
    "    k = len(model.init_probs)\n",
    "    n = len(x)\n",
    "    x = translate_observations_to_indices(x)\n",
    "    \n",
    "    w = make_table(k, n)\n",
    "    #print(w)\n",
    "    # Base case: fill out w[i][0] for i = 0..k-1\n",
    "    for j in range(k):\n",
    "        w[j][0] = model.init_probs[j]*model.emission_probs[j][x[0]]\n",
    "    # ...\n",
    "    \n",
    "    # Inductive case: fill out w[i][j] for i = 0..k, j = 0..n-1\n",
    "    for i in range(1, n): #kolonnerne\n",
    "        for j in range(k): #rækkerne #den k vi er i\n",
    "            max_prob = 0\n",
    "            for l in range(k): #den k vi kommer fra\n",
    "                prob = w[l][i-1]*model.trans_probs[l][j]*model.emission_probs[j][x[i]] #den øverste pil\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "            w[j][i] = max_prob\n",
    "    return w\n",
    "                \n",
    "print(compute_w(hmm_7_state, 'GTTTAGC'))            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the joint probability of an optimal path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function that given the $\\omega$-table, returns the probability of an optimal path through the HMM. As explained in the lecture, this corresponds to finding the highest probability in the last column of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.243658447265626e-05\n"
     ]
    }
   ],
   "source": [
    "def opt_path_prob(w):\n",
    "    max_prob = 0\n",
    "    for j in range(len(w)):\n",
    "        prob = w[j][-1]\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "    return max_prob\n",
    "\n",
    "table_test=compute_w(hmm_7_state, 'GTTTAGC')\n",
    "print(opt_path_prob(table_test))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your implementation in the box below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9114255184318858e-31"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w(hmm_7_state, x_short)\n",
    "opt_path_prob(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = compute_w(hmm_7_state, x_long)\n",
    "opt_path_prob(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining an optimal path through backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement backtracking to find a most probable path of hidden states given the $\\omega$-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backtrack(model, x, w):\n",
    "    backtrack_list = []\n",
    "    x = translate_observations_to_indices(x)\n",
    "    \n",
    "    max_prob = 0\n",
    "    max_prob_position = None \n",
    "    for j in range(len(w)):\n",
    "        prob = w[j][-1]\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_prob_position = j\n",
    "    backtrack_list.append(max_prob_position)\n",
    "\n",
    "    pre_prob = max_prob\n",
    "    pre_prob_position = max_prob_position\n",
    "    \n",
    "    for i in range(-2, -len(x), -1):\n",
    "        for j in range(len(w)):\n",
    "            prob = w[j][i]*model.trans_probs[j][pre_prob_position]*model.emission_probs[pre_prob_position][x[i+1]]\n",
    "            if math.isclose(prob,  pre_prob):\n",
    "                pre_prob = w[j][i]\n",
    "                pre_prob_position = j\n",
    "                backtrack_list.append(pre_prob_position)\n",
    "\n",
    "    return backtrack_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "w = compute_w(hmm_7_state, x_short)\n",
    "z_viterbi = backtrack(hmm_7_state, x_short, w)\n",
    "print(z_viterbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing with log-transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the Viterbi algorithm with log-transformation. The steps are the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the (log-transformed) $\\omega$ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_w_log(model, x):\n",
    "    k = len(model.init_probs)\n",
    "    n = len(x)\n",
    "    \n",
    "    w = make_table(k, n)\n",
    "    \n",
    "    # Base case: fill out w[i][0] for i = 0..k-1\n",
    "    # ...\n",
    "    \n",
    "    # Inductive case: fill out w[i][j] for i = 0..k, j = 0..n-1\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # Just ignore this :-)\n",
    "\n",
    "def log(x):\n",
    "    if x == 0:\n",
    "        return float('-inf')\n",
    "    return math.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the (log-transformed) joint probability of an optimal path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_path_prob_log(w):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = compute_w_log(hmm_7_state, x_short)\n",
    "opt_path_prob_log(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining an optimal path through backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backtrack_log(model, x, w):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = compute_w_log(hmm_7_state, x_short)\n",
    "z_viterbi_log = backtrack_log(hmm_7_state, x_short, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for `x_long`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about how to verify that your implementations of Viterbi (i.e. `compute_w`, `opt_path_prob`, `backtrack`, and there log-transformed variants `compute_w_log`, `opt_path_prob_log`, `backtrack_log`) are correct.\n",
    "\n",
    "One thing that should hold is that the probability of a most likely path as computed by `opt_path_prob` (or `opt_path_prob_log`) for a given sequence of observables (e.g. `x_short` or `x_long`) should be equal to the joint probability of a corersponding most probable path as found by `backtrack` (or `backtrack_log`) and the given sequence of observables. Why?\n",
    "\n",
    "Make an experiment that validates that this is the case for your implementations of Viterbi and `x_short` and `x_long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To access joint_prob and joint_prob_log, you must copy your implementations from last week here ...\n",
    "\n",
    "def joint_prob(model, x, z):\n",
    "    pass\n",
    "\n",
    "def joint_prob_log(model, x, z):\n",
    "    pass\n",
    "\n",
    "# Check that opt_path_prob is equal to joint_prob(hmm_7_state, x_short, z_viterbi)\n",
    "\n",
    "# Your code here ...\n",
    "\n",
    "# Check that opt_path_prob_log is equal to joint_prob_log(hmm_7_state, x_short, z_viterbi_log)\n",
    "\n",
    "# Your code here ...\n",
    "\n",
    "# Do the above checks for x_long ...\n",
    "\n",
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your implementations pass the above checks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does log-transformation matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an experiment that investigates how long the input string can be before `backtrack` and `backtrack_log` start to disagree on a most likely path and its probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**\n",
    "\n",
    "For the 7-state model, `backtrack` and `backtrack_log` start to disagree on a most likely path and its probability for **i = ?** ."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
